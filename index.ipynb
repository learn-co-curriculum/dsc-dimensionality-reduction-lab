{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Dimensionality Reduction - Cumulative Lab\n", "\n", "## Introduction\n", "\n", "In this cumulative lab, you'll apply dimensionality reduction as a preprocessing step in a machine learning workflow.\n", "\n", "## Objectives\n", "\n", "You will be able to: \n", "\n", "- Practice performing PCA using the scikit-learn library\n", "- Interpret the amount of variance captured by a given number of PCA components\n", "- Evaluate model performance with and without dimensionality reduction\n", "- Plot the decision boundary of classification experiments to visually inspect their performance "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Your Task: Reduce the Dimensionality of the Iris Dataset as Part of a Machine Learning Workflow\n", "\n", "![irises](iris.jpg)\n", "\n", "<span>Photo by <a href=\"https://unsplash.com/@yoksel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Yoksel \ud83c\udf3f Zok</a> on <a href=\"https://unsplash.com/s/photos/iris?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dimensionality Reduction in ML\n", "\n", "While it is possible to use dimensionality reduction as a standalone analysis technique, you will frequently see it used as a preprocessing step in a predictive machine learning workflow.\n", "\n", "The two main reasons to use dimensionality reduction in machine learning are:\n", "\n", "1. **Reducing computational complexity:** Often the internal logic of a machine learning algorithm means that the complexity increases by an order of magnitude with every additional dimension (feature). So maybe there are {n^2} operations for 2 features, {n^4} operations for 4 features, etc. If we can reduce the number of dimensions (features) prior to fitting/predicting with the model, the model will be faster and use fewer computational resources (memory, processing power, etc.)\n", "2. **Improving model performance:** In some cases even if we had unlimited computational capacity, our models would still struggle to fit on data with too many dimensions, known as the *curse of dimensionality*. Generally this applies when there are hundreds of features (or more). We can also sometimes see reductions in overfitting with dimensionality reduction, depending on the data and the model.\n", "\n", "There is no guarantee that dimensionality reduction will produce improved results \u2014 it all depends on how your features are related to each other, and the details of the machine learning algorithm you are using. In this lab you will walk through several different dimensionality reduction techniques and observe their impacts on the modeling process.\n", "\n", "### The Iris Dataset\n", "\n", "For this lab you will use the Iris Dataset that comes with scikit-learn. This is a classic \"toy\" dataset where we are trying to identify the species of iris based on the provided attributes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn import datasets\n", "import pandas as pd\n", " \n", "iris = datasets.load_iris()\n", "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n", "df['target'] = iris.get('target')\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Part of why we use this dataset for so many examples is that there is clear predictive power in each of the features (i.e. the distributions of feature values differ for each of the targets):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "# Set up figure and axes\n", "fig, axes = plt.subplots(ncols=4, figsize=(15,4))\n", "\n", "# Loop over each feature\n", "for feature_index, feature_name in enumerate(iris.feature_names):\n", "    \n", "    # Find the relevant axes from the list and give it a label\n", "    ax = axes[feature_index]\n", "    ax.set_xlabel(feature_name)\n", "    \n", "    # Loop over the three target values and plot them by color\n", "    colors = ['r', 'g', 'b']\n", "    for target_index, target_name in enumerate(iris.target_names):\n", "        subset_rows = df[df[\"target\"] == target_index]\n", "        ax.hist(\n", "            subset_rows[feature_name],\n", "            label=target_name,    \n", "            color=colors[target_index],\n", "            alpha=0.5\n", "        )\n", "        \n", "# Add a legend to the last axes only\n", "ax.legend()\n", "\n", "# Set a title for the overall plot\n", "fig.suptitle(\"Feature Distributions by Iris Type\");"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Requirements\n", "\n", "#### 1. Perform a Train-Test Split\n", "\n", "Because we are using dimensionality reduction within a predictive modeling context, we need to perform a train-test split prior to taking any other steps.\n", "\n", "#### 2. Scale Data\n", "\n", "Both the model we are using (logistic regression with regularization) and our dimensionality reduction techniques are distance-based, so we need to scale our data before performing any analysis.\n", "\n", "#### 3. Evaluate Model Performance without PCA\n", "\n", "Before performing PCA, fit a vanilla logistic regression model on the provided features and evaluate its performance, including the time taken.\n", "\n", "#### 4. Perform and Visualize PCA\n", "\n", "Using the `PCA` transformer class from scikit-learn, fit and transform the training data so that the four dimensions of the original features have been projected down to two dimensions. Identify how much of the variance is captured, and plot the data points using these two dimensions as the x-axis and y-axis.\n", "\n", "#### 5. Evaluate Model Performance with PCA\n", "\n", "Fit and evaluate a new logistic regression model on the transformed data.\n", "\n", "#### BONUS: Manifold Dimensionality Reduction\n", "\n", "Another, more-advanced technique to consider for dimensionality reduction is *manifold learning*. Fortunately scikit-learn also provides an interface to this technique that works the same way as any other transformer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Perform a Train-Test Split\n", "\n", "Currently all of the data is contained in a dataframe called `df`, where the target (y value) is labeled `\"target\"`. In the cell below, separate the data into `X` and `y`, then use `train_test_split` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) with `random_state=42` to create training and test datasets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Import the relevant function\n", "None\n", "\n", "# Separate X and y\n", "X = None\n", "y = None\n", "\n", "# Perform train-test split with random_state=42\n", "X_train, X_test, y_train, y_test = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make sure your data has the appropriate shape before moving forward:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Both train and test X should have 4 columns\n", "assert (X_train.shape[1] == 4) and (X_test.shape[1] == 4)\n", "\n", "# Both train and test y should have 1 column\n", "assert (len(y_train.shape) == 1) and (len(y_test.shape) == 1)\n", "\n", "# Train X and y should have the same number of rows\n", "assert X_train.shape[0] == y_train.shape[0]\n", "\n", "# Test X and y should have the same number of rows\n", "assert X_test.shape[0] == y_test.shape[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Scale Data\n", "\n", "Use the `StandardScaler` class from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) to preprocess the data. Make sure you fit the scaler on the training data only, and transform both the train and test data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# Instantiate a scaler\n", "scaler = None\n", "\n", "# Fit the scaler on X_train\n", "None\n", "\n", "# Transform X_train and X_test. Go ahead and reuse the variable names \n", "# \"X_train\" and \"X_test\" since we won't need the un-scaled data\n", "None\n", "None\n", "\n", "# Now the values should be scaled\n", "pd.DataFrame(X_train, columns=iris.feature_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Evaluate Model Performance without PCA\n", "\n", "In the cell below, instantiate a `LogisticRegression` model ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) with `random_state=42`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Import the relevant class\n", "None\n", "\n", "# Instantiate the model with random_state=42\n", "baseline_model = None\n", "baseline_model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now fit the model on the training data and score it on the test data (using the `.score` method)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "import time\n", "\n", "start = time.time()\n", "\n", "# Fit the model on the training data\n", "None\n", "\n", "# Score the model on the test data\n", "baseline_model_score = None\n", "\n", "end = time.time()\n", "baseline_time_taken = end - start\n", "\n", "print(\"Model score:\", baseline_model_score)\n", "print(\"Time taken:\", baseline_time_taken)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "baseline_model_score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, so it looks like we are getting essentially perfect performance with our baseline classifier, and it is taking aroud 8ms (timing will vary depending on your system).\n", "\n", "Now let's investigate using PCA to reduce the dimensions of the data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Perform and Visualize PCA\n", "\n", "### Performing PCA\n", "\n", "As demonstrated in a previous lesson, PCA can be applied relatively simply using the scikit-learn library. Just like with the `StandardScaler`, you'll need to instantiate a `PCA` transformer, fit it on the training data, and transform both the train and the test data. You can find documentation for the `PCA` class [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n", "\n", "We'll set `n_components` to `2`, meaning that we will only keep the first two principal components."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Import the relevant class\n", "None\n", "\n", "# Instantiate the PCA transformer with n_components=2\n", "pca = None\n", "\n", "# Fit the transformer on X_train\n", "None\n", "\n", "# Transform X_train and X_test. This time, create new\n", "# variables for the transformed data\n", "X_train_pca = None\n", "X_test_pca = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, our PCA-transformed X values should have the same number of rows as before, but a different number of columns:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Same number of rows, different number of columns\n", "assert X_train_pca.shape[0] == X_train.shape[0]\n", "assert X_train_pca.shape[1] != X_train.shape[1]\n", "\n", "# Specifically, the transformed data should have 2 columns\n", "# because we set n_components=2\n", "assert X_train_pca.shape[1] == 2\n", "\n", "pd.DataFrame(X_train_pca, columns=[\"PC 1\", \"PC 2\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualizing Principal Components "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using the target data, we can visualize the principal components according to the class distribution. \n", "\n", "Create a scatter plot from principal components, where the color of the dot is based on the target value.\n", "\n", "First, separate `X_train_pca` based on the associated target value in `y_train`. Create dataframes `setosa` (target = 0), `versicolor` (target = 1), and `virginica` (target = 2) using the dataframe created below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "train_combined_pca = pd.DataFrame(X_train_pca, columns=[\"PC 1\", \"PC 2\"])\n", "train_combined_pca[\"target\"] = y_train.values\n", "train_combined_pca"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Rows of train_combined_pca where target is 0\n", "setosa = None\n", "\n", "# Rows of train_combined_pca where target is 1\n", "versicolor = None\n", "\n", "# Rows of train_combined_pca where target is 2\n", "virginica = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The code below checks that the dataframes have the correct length:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "train_value_counts = y_train.value_counts()\n", "\n", "assert len(setosa) == train_value_counts[0]\n", "assert len(versicolor) == train_value_counts[1]\n", "assert len(virginica) == train_value_counts[2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can set up the actual scatter plots.\n", "\n", "Notes:\n", "\n", "* You'll need to call `ax.scatter` three times, once with each of the types of iris.\n", "* The x values passed in should be the values from the `\"PC 1\"` column, whereas the y values should be the values from the `\"PC 2\"` column.\n", "* Set the color of `setosa` to red using `c='r'`, `versicolor` to green using `c='g'`, and `virginica` to blue using `c='b'`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Set up figure and axes\n", "fig, ax = plt.subplots(figsize=(10, 8))\n", "ax.grid()\n", "\n", "# Scatter plot of setosa (red)\n", "None\n", "\n", "# Scatter plot of versicolor (green)\n", "None\n", "\n", "# Scatter plot of virginica (blue)\n", "None\n", "\n", "# Customize labels\n", "ax.set_xlabel('First Principal Component ', fontsize = 15)\n", "ax.set_ylabel('Second Principal Component ', fontsize = 15)\n", "ax.set_title('Principal Component Analysis (2 PCs) for Iris Dataset', fontsize = 20)\n", "ax.legend(iris.target_names, fontsize=\"large\");"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Explained Variance\n", "\n", "You can see above that the three classes in the dataset are fairly well separable, even though the data has been projected into two dimensions (down from 4 dimensions). As such, this compressed representation of the data is probably sufficient for the classification task at hand.\n", "\n", "Let's confirm this evaluation by extracting the explained variance ratio from the fitted `pca` object. It has an attribute `explained_variance_ratio_` that describes the amount of variance explained by each principal component. (Remember you can find the full PCA documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).)\n", "\n", "In the cell below, extract that information from `pca`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Extract the explained variance ratio from the pca object\n", "evr_all_components = None\n", "\n", "pc1_evr = evr_all_components[0]\n", "pc2_evr = evr_all_components[1]\n", "print(f\"The first principal component explains {round(pc1_evr*100, 3)}% of the variance\")\n", "print(f\"The second principal component explains {round(pc2_evr*100, 3)}% of the variance\")\n", "print()\n", "print(f\"Overall, the first two principal components explain {round(sum(evr_all_components*100), 3)}% of the variance\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, these first two principal components account for the vast majority of the overall variance in the dataset. This is indicative of the total information encapsulated in the compressed representation (2 dimensions) compared to the original encoding (4 dimensions)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Evaluate Model Performance with PCA\n", "\n", "Since the first two principal components explain 95% of the variance in the data, we are hoping that the model performance will be similar on the lower-dimensional data, while improving computational speed.\n", "\n", "In the cell below, we instantiate a new logistic regression model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "pca_model = LogisticRegression(random_state=42)\n", "pca_model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, fit the model on `X_train_pca` instead of `X_train`, and evaluate it on `X_test_pca` instead of `X_test`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "start = time.time()\n", "\n", "# Fit the model on the training data\n", "None\n", "\n", "# Score the model on the test data\n", "pca_model_score = None\n", "\n", "end = time.time()\n", "pca_time_taken = end - start\n", "\n", "print(\"Model score with PCA:\", pca_model_score)\n", "print(\"Baseline model score:\", baseline_model_score)\n", "print()\n", "print(\"Time taken with PCA:\", pca_time_taken)\n", "print(\"Baseline time taken:\", baseline_time_taken)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Although some accuracy is lost in this representation of the data, we were able to use half of the number of features to train the model! On average, this results in faster model fitting and predicting speeds.\n", "\n", "(Comparing execution time is inexact since it is based on the CPU load as well as all of the current processes running on your computer at the time, but generally you should see a lower time taken with PCA than the baseline. Try re-running the model fitting cells a couple times if you aren't seeing this.)\n", "\n", "This use case is a bit contrived (reducing 4 dimensions to 2 dimensions), but you can imagine how this might work if you reduced hundreds of dimensions down this way. We also are not seeing evidence of overfitting right now, but this could also help with overfitting on another dataset."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualizing the Decision Boundaries\n", "\n", "One other useful aspect of applying PCA to reduce data to two dimensions is that it allows us to visualize our model's decision boundaries in two dimensions. Run the cell below to visualize how our model uses the two principal components:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import numpy as np\n", "\n", "# Combine all data into one df\n", "X_all_pca = pd.concat([pd.DataFrame(X_train_pca), pd.DataFrame(X_test_pca)], axis=0)\n", "X_all_pca.columns = [\"PC 1\", \"PC 2\"]\n", "X_all_pca[\"target\"] = pd.concat([y_train, y_test], axis=0).values\n", "\n", "# Set up figure and axes\n", "fig, ax = plt.subplots(figsize=(10, 8))\n", "ax.grid()\n", "\n", "# Scatter plot of all data points\n", "colors = [\"r\", \"g\", \"b\"] # Setting up colors again in case they were edited earlier\n", "for target_index in [0, 1, 2]:\n", "    subset_rows = X_all_pca[X_all_pca[\"target\"] == target_index]\n", "    ax.scatter(subset_rows[\"PC 1\"], subset_rows[\"PC 2\"], c=colors[target_index])\n", "\n", "# Get bounds and set step size for mesh grid\n", "x_min, x_max = ax.get_xlim()\n", "y_min, y_max = ax.get_ylim()\n", "h = 0.01\n", "\n", "# Build mesh grid\n", "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n", "Z = pca_model.predict(np.c_[xx.ravel(), yy.ravel()])\n", "Z = Z.reshape(xx.shape)\n", "\n", "# Plot filled contour\n", "ax.contourf(xx, yy, Z, alpha=0.25, cmap=\"gray\")\n", "\n", "# Customize labels\n", "ax.set_xlabel('First Principal Component ', fontsize = 15)\n", "ax.set_ylabel('Second Principal Component ', fontsize = 15)\n", "ax.set_title('PCA-Transformed Iris Dataset with Decision Boundaries', fontsize = 20)\n", "ax.legend(iris.target_names, fontsize=15);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## BONUS: Manifold Dimensionality Reduction\n", "\n", "As mentioned previously, PCA is not the only technique for dimensionality reduction, although it is the only dimensionality reduction technique described in depth in our curriculum.\n", "\n", "Another major form of dimensionality reduction is *manifold learning*, which you can read about more in depth [here](https://scikit-learn.org/stable/modules/manifold.html).\n", "\n", "Here we'll have a quick demo of one type of manifold learning, isomap embedding ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html)). Similar to PCA, isomap embedding creates components that can be plotted in 2D space."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.manifold import Isomap\n", "\n", "iso = Isomap(n_components=2)\n", "\n", "iso.fit(X_train)\n", "\n", "X_train_iso = iso.transform(X_train)\n", "X_test_iso = iso.transform(X_test)\n", "\n", "iso_model = LogisticRegression()\n", "\n", "start = time.time()\n", "\n", "iso_model.fit(X_train_iso, y_train)\n", "iso_model_score = iso_model.score(X_test_iso, y_test)\n", "\n", "end = time.time()\n", "iso_time_taken = end - start\n", "\n", "print(\"Model score with iso:\", iso_model_score)\n", "print(\"Model score with PCA:\", pca_model_score)\n", "print(\"Baseline model score:\", baseline_model_score)\n", "print()\n", "print(\"Time taken with iso:\", iso_time_taken)\n", "print(\"Time taken with PCA:\", pca_time_taken)\n", "print(\"Baseline time taken:\", baseline_time_taken)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, preprocessing with isomap embedding gets us slightly better performance than preprocessing with PCA with this particular dataset + model, while still being faster than the baseline.\n", "\n", "This plot shows the components from the isomap embedding as well as the new decision boundaries:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Combine all data into one df\n", "X_all_iso = pd.concat([pd.DataFrame(X_train_iso), pd.DataFrame(X_test_iso)], axis=0)\n", "X_all_iso.columns = [\"Component 1\", \"Component 2\"]\n", "X_all_iso[\"target\"] = pd.concat([y_train, y_test], axis=0).values\n", "\n", "# Set up figure and axes\n", "fig, ax = plt.subplots(figsize=(10, 8))\n", "ax.grid()\n", "\n", "# Scatter plot of all data points\n", "colors = [\"r\", \"g\", \"b\"] # Setting up colors again in case they were edited earlier\n", "for target_index in [0, 1, 2]:\n", "    subset_rows = X_all_iso[X_all_iso[\"target\"] == target_index]\n", "    ax.scatter(subset_rows[\"Component 1\"], subset_rows[\"Component 2\"], c=colors[target_index])\n", "\n", "# Get bounds and set step size for mesh grid\n", "x_min, x_max = ax.get_xlim()\n", "y_min, y_max = ax.get_ylim()\n", "h = 0.01\n", "\n", "# Build mesh grid\n", "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n", "Z = iso_model.predict(np.c_[xx.ravel(), yy.ravel()])\n", "Z = Z.reshape(xx.shape)\n", "\n", "# Plot filled contour\n", "ax.contourf(xx, yy, Z, alpha=0.25, cmap=\"Greys\")\n", "\n", "# Customize labels\n", "ax.set_xlabel('First Component ', fontsize = 15)\n", "ax.set_ylabel('Second Component ', fontsize = 15)\n", "ax.set_title('Isometric Embedded Iris Dataset with Decision Boundaries', fontsize = 20)\n", "ax.legend(iris.target_names, fontsize=15);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary \n", "\n", "In this lab, you applied PCA to the popular Iris Dataset in the context of a machine learning workflow. You looked at the performance of a simple classifier and the impact of PCA on the accuracy of the model and the time it took to run the model. You also used both PCA and isometric embedding to view higher-dimensional data in two dimensions, as well as the associated classifier decision boundaries."]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python (python3)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.5"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 1}